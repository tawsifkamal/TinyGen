{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.llms import Bedrock\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from utils import get_files_and_contents\n",
    "from getpass import getpass\n",
    "\n",
    "\n",
    "# ACCESS_TOKEN = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_repo = get_files_and_contents(\"llm.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_user_query = r\"\"\" \n",
    "(base) C:\\Users\\off99\\Documents\\Code\\>llm list files in current dir; windows\n",
    "/ Querying GPT-3200\n",
    "───────┬────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
    "       │ File: temp.sh\n",
    "───────┼────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
    "   1   │\n",
    "   2   │ dir\n",
    "   3   │ ```\n",
    "───────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
    ">> Do you want to run this program? [Y/n] y\n",
    "\n",
    "Running...\n",
    "\n",
    "\n",
    "(base) C:\\Users\\off99\\Documents\\Code\\>\n",
    "Notice that there is no output. Is this supposed to work on Windows also?\n",
    "Also it might be great if the script detects which OS or shell I'm using and try to use the appropriate command e.g. dir instead of ls because I don't want to be adding windows after every prompt.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4-0125-preview\")\n",
    "# llm = Bedrock(model_id=\"anthropic.claude-v2:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixing_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert at generating Git Diffs. Given a user query defined under <user_query><user_query/>,\n",
    "    you can identify the files that you need to update to solve the problem/issue that the user is having. \n",
    "    These files are given as context to you in between <repository></repository>.\n",
    "\n",
    "    <repository>\n",
    "    {repository}\n",
    "    </repository>\n",
    "\n",
    "    Given these files, do the following step by step:\n",
    "    1. Identify what files need to be changed to solve user's query\n",
    "    2. Identify what changes need to be made in the files\n",
    "    3. generate a git diff showing all of the changes that you have made\n",
    "    4. lastly, check your work to make sure these are the changes you would like to make to solve the user's query\n",
    "\n",
    "    Make sure to only return the diff and no other text.\n",
    "    \n",
    "    <user_query>\n",
    "    {user_query}\n",
    "    </user_query>\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_prompt = PromptTemplate.from_template(\n",
    "    \"\"\" \n",
    "    The user wants to update the functionality of the code within the given repository and you have generated the git diff\n",
    "    for the code changes located in <diff></diff>. The issue that the user was facing is located below in <user_query></user_query>. \n",
    "    Additionally, the repository that the user is referencing is loccated in <repository></repository>. The repository section contains\n",
    "    all of the files. You can use it to verify if it is updating the correct code section and correct file\n",
    "\n",
    "\n",
    "    <user_query>\n",
    "    {user_query}\n",
    "    </user_query>\n",
    "\n",
    "    <diff>\n",
    "    {diff}\n",
    "    </diff>\n",
    "\n",
    "    <repository>\n",
    "    {repository}\n",
    "    </repository>\n",
    "\n",
    "    Now, are you sure these are the changes that you want to make in the code? Are you positive that the code changes within the git diff will solve the user's query? If you want to correct\n",
    "    your answer, then generate a new git diff and then return it!\n",
    "\n",
    "    Only return the git diff and no other text.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_diff_chain = (\n",
    "    {\n",
    "        \"diff\": (\n",
    "            RunnablePassthrough.assign(\n",
    "                repository=lambda x: files_in_repo\n",
    "            )\n",
    "            | fixing_prompt\n",
    "            | llm\n",
    "            | StrOutputParser() \n",
    "        ),\n",
    "        \"repository\": lambda x: files_in_repo,\n",
    "        \"user_query\": RunnablePassthrough(),\n",
    "    }\n",
    ")\n",
    "\n",
    "reflection_chain = reflection_prompt | llm | StrOutputParser()\n",
    "\n",
    "final_chain = generate_diff_chain | reflection_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = final_chain.invoke({\"user_query\": sample_user_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```diff\n",
      "diff --git a/llm.sh/src/main.py b/llm.sh/src/main.py\n",
      "index d7e5263..b4a1f87 100644\n",
      "--- a/llm.sh/src/main.py\n",
      "+++ b/llm.sh/src/main.py\n",
      "@@ -1,5 +1,6 @@\n",
      " import os\n",
      " import sys\n",
      "+import platform\n",
      " from colorama import Fore\n",
      " from halo import Halo\n",
      " import requests\n",
      "@@ -38,6 +39,11 @@ def model_query(prompt: str) -> str:\n",
      "     return response.json()['text']\n",
      " \n",
      " \n",
      "+def get_os_command(command: str) -> str:\n",
      "+    if platform.system() == \"Windows\":\n",
      "+        return command.replace(\"ls\", \"dir\")\n",
      "+    return command\n",
      "+\n",
      " def process():\n",
      "     prompt = ' '.join(sys.argv[1:])\n",
      "     result = model_query(prompt)\n",
      "@@ -45,6 +51,7 @@ def process():\n",
      "     response = input(Fore.RED + '>> Do you want to run this program? [Y/n] ')\n",
      "     if response == '' or response.lower() == 'y':\n",
      "         print(Fore.LIGHTBLACK_EX + '\\nRunning...\\n')\n",
      "+        result = get_os_command(result)\n",
      "         run_bash_file_from_string(result)\n",
      "     else:\n",
      "         print(Fore.LIGHTBLACK_EX + 'Aborted.')\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```diff\n",
    "diff --git a/llm.sh/src/main.py b/llm.sh/src/main.py\n",
    "index d7e5263..b4a1f87 100644\n",
    "--- a/llm.sh/src/main.py\n",
    "+++ b/llm.sh/src/main.py\n",
    "@@ -1,5 +1,6 @@\n",
    " import os\n",
    " import sys\n",
    "+import platform\n",
    " from colorama import Fore\n",
    " from halo import Halo\n",
    " import requests\n",
    "@@ -38,6 +39,11 @@ def model_query(prompt: str) -> str:\n",
    "     return response.json()['text']\n",
    " \n",
    " \n",
    "+def get_os_command(command: str) -> str:\n",
    "+    if platform.system() == \"Windows\":\n",
    "+        return command.replace(\"ls\", \"dir\")\n",
    "+    return command\n",
    "+\n",
    " def process():\n",
    "     prompt = ' '.join(sys.argv[1:])\n",
    "     result = model_query(prompt)\n",
    "@@ -45,6 +51,7 @@ def process():\n",
    "     response = input(Fore.RED + '>> Do you want to run this program? [Y/n] ')\n",
    "     if response == '' or response.lower() == 'y':\n",
    "         print(Fore.LIGHTBLACK_EX + '\\nRunning...\\n')\n",
    "+        result = get_os_command(result)\n",
    "         run_bash_file_from_string(result)\n",
    "     else:\n",
    "         print(Fore.LIGHTBLACK_EX + 'Aborted.')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Github Access Token currently not working right now\n",
    "\n",
    "GITHUB_URL = \"octokit/octokit.rb\"\n",
    "\n",
    "from langchain_community.document_loaders import GithubFileLoader\n",
    "\n",
    "loader = GithubFileLoader(\n",
    "    repo=GITHUB_URL,  # the repo name\n",
    "    access_token=ACCESS_TOKEN,\n",
    "    github_api_url=\"https://api.github.com\",\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyGen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
